---
title: "PlayingWithData"
author: "Colleen Minnihan"
date: "3/29/2021"
output: html_document
---

```{r message = FALSE, warning = FALSE}
# load libraries 
library(dplyr)
library(tidytext)
library(tidyr)
library(tidyverse)
library(ggplot2)
# for word cloud
library(textdata)
library(reshape2)
library(wordcloud)
```

```{r}
# read in data sets
# Download file from github and place in same folder

troll_tweets<- read.csv("IRAhandle_tweets_12.csv")

```

```{r}



text<-c("Lisa is awesome","I love Data Science","I hate cilantro", "I dislike vegetables", "I enjoy smiling", "I hate exams", "I love travelling")
text

```
```{r}
text_df<- tibble(line = 1:length(text), text = text)
text_df
```

```{r}
text_unnest <- text_df %>% 
  unnest_tokens(word, text)
```


```{r}
head(stop_words) #write description about this

text_clean <- text_unnest %>% 
 anti_join(stop_words) 

text_clean
```

```{r}
text_clean %>%
  inner_join(get_sentiments("bing")) 

text_clean %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("red", "green"),
                   max.words = 100)
```
```{r}
text_clean %>%
  inner_join(get_sentiments("bing")) %>% 
  count(sentiment)
```
```{r}

text_count<- text_unnest %>% 
  count(word)
text_count
text_clean

text_clean %>% 
  left_join(text_count, 
            by = "word") %>% 
  bind_tf_idf(word, line, n)
```


**Data Exploration: size, distribution, etc**

```{r}
# look at the dimensions
dim(troll_tweets)


```

**Q: Data Cleaning**

```{r}
# only consider english data 
troll_tweets1 <- troll_tweets%>%
  filter(language == 'English') #only tweets that are in English
  

```

```{r}
troll_tweets1 %>% 
  select(where(is.numeric)) %>% 
  pivot_longer(cols = everything(),
               names_to = "variable", 
               values_to = "value") %>% 
  ggplot(aes(x = value)) +
  geom_bar() +
  facet_wrap(vars(variable), 
             scales = "free")
```


```{r}
#separate tweet so that each row gets an individual word: unnest tokens
troll_tweets_untoken <- troll_tweets1 %>%
  unnest_tokens(word,content)

troll_tweets_untoken

#get rid of stopwords (the, and, etc.)
troll_tweets_cleaned <- troll_tweets_untoken %>%
  anti_join(stop_words) %>%
  filter(word != 'https', word != 't.co') #get rid of https and t.co

# top 100 words 
troll_tweets_cleaned %>%
  count(word) %>%
  slice_max(order_by = n, n = 100)
```

```{r}
# Final cleaning stage: get rid of 2, rt, 1, amp, 3, http, 5, 4, 10, and single letter words
troll_tweets_fullclean <- troll_tweets_cleaned %>%
  mutate(word = str_extract(word,"[a-z']+")) %>%
  filter(word != '1', word != '2', word != '3', word != '4', word != '5', word != '10', word != 'rt', word != 'amp', word != 'http', !(word %in% letters))
```

```{r}
troll_tweets_small <- troll_tweets_fullclean %>%
  count(word) %>%
  slice_max(order_by = n, n = 50)

ggplot(troll_tweets_small, aes(y = fct_reorder(word,n), x = n)) +
  geom_col()
```

**Q: Make wordcloud**
```{r message = FALSE, warning = FALSE}
troll_tweets_100 <- troll_tweets_fullclean %>%
  count(word) %>%
  slice_max(order_by = n, n = 100)

# most frequently used words
troll_tweets_100 %>%
  with(wordcloud(word, n, max.words = 100))

wordcloud(words = troll_tweets_100$word, freq = troll_tweets_100$n, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))


# sentiment analysis (positive vs negative)


```

**Q: More Sentiment Analysis**

```{r}
# bing makes it into positive / negative
troll_tweets_sentiment <- troll_tweets_fullclean %>%
  inner_join(get_sentiments("bing")) %>%
  count(sentiment)

#count negative and positive words
troll_tweets_sentiment

get_sentiments("bing")
```




