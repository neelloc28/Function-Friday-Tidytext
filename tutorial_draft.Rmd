---
title: "PlayingWithData"
author: "Colleen Minnihan"
date: "3/29/2021"
output: html_document
---

### **Introduction** 

Today, we will talk about the `TidyText` package. The `tidytext` packages allows us to effectively do our text mining tasks by converting texts into *tidy formats*, when incorporated with other tools in data science or machine learning. 

In this tutorial, we will explain some of the key functions used in the 'tidytext' package by using a simple example. We will then ask you to work on questions using a different data set: Russian Troll Tweets. 


\
\
\


### **Getting Started** 

**Loading libraries**

First, as always, we will have to load libraries and install packages. You will encounter some familiar packages, but the main libraries required to do text mining with `tidytext` are listed below: 

```{r message = FALSE, warning = FALSE}
# load libraries 
library(dplyr)
library(tidytext)
library(tidyr)
library(tidyverse)
library(ggplot2)
library(textdata)
library(reshape2)
library(wordcloud) # for wordcloud
library(stopwords)
```



**Looking at the Data Set**

After loading all the relevant libraries, we will look a simple data set that we created. The `text` data set includes 9 different sentences or strings. 

```{r}
text<-c("Lisa is awesome","I love Data Science","I hate cilantro", "I dislike vegetables", "I enjoy smiling", "I hate exams", "I love travelling", "The weather is so nice!", "Can I have an apple?")

text
```


The first task before doing going into text analysis is converting our data set into a *tibble* data frame by using the `tibble` function. A *tibble* is just another class of data frames in R, which allows us to work with tidy functions as it does not convert strings to factors and does not use row names. 

Take a look at how we were able to achieve this below: 

```{r}
# Converting to a tibble data frame 
text_df<- tibble(line = 1:length(text), text = text)

# Tibble data frame
text_df
```

After we successfully convert our data set into a tibble data frame, we want to extract individual words and put them into a data frame so that each row has one *token*. 

Each `token` can be interpreted as a word or a unit of text. By utilizing tidytext's `unnest_tokens` function, we can break the text into individual tokens. This process is also known as the *tokenization* process. Note that this function eliminates all the punctuation marks.    

```{r}
# Tokenization process
text_unnest <- text_df %>% 
  unnest_tokens(word, text)

# Tokenized data 
text_unnest
```

As we see above, the `unnest_tokens` function takes in two parameters. The first parameter, the output column, will be the name of the output column or individual words. In our exercise, we will call it `word`. 

The second parameter will take the tokenized column name from the tibble data frame.

Before starting our text analysis, we will also have to exclude stop words such as "is", "I", "the", "about", "an", "the", etc. The `tidytext` package provides a tibble data set called `stop_words` that includes different stop words. 

You can take a peak at some of the stop words by calling the `stop_words` data set:    

```{r}
# Look at the stop words data set
head(stop_words)
```


Finally, we will use a familiar function, `anti_join`, to clean the data set. This is the last step of the data cleaning process that we need to do before moving forward with the analysis. By joining the two data sets, `stop_words` and `text_unnest`, we can arrive at a clean data set. 

```{r}
# Join the two data sets together to only extract meaningful words
text_clean <- text_unnest %>% 
 anti_join(stop_words) 

# Cleaned data set 
text_clean
```


\
\
\

### **Data Analysis**

With the cleaned version of the data, we will begin our analysis. In this tutorial, we mainly focus on the *sentiment analysis* and creating relevant visualizations using *word cloud*  


**Sentiment Analysis**

Before beginning the sentiment analysis, let's take a step back and understand the purpose of the sentiment analysis. Sentiment analysis "provides a way to understand the attitudes and opinions expressed in texts". 

To conduct sentiment analysis, we will be first looking at the `get_sentiments()`. By using this package   

```{r}
# Sentiments 
sentiments <- get_sentiments("bing")

# Negative words
sentiments %>% 
  filter(sentiment == "negative") %>% 
  head()

# Positive words
sentiments %>% 
  filter(sentiment == "positive") %>% 
  head()
```


```{r}
text_clean %>%
  inner_join(get_sentiments("bing")) 
```

```{r}
text_clean %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("red", "green"),
                   max.words = 100)
```

```{r}
text_clean %>%
  inner_join(get_sentiments("bing")) %>% 
  count(sentiment)
```







**Read in troll tweets data**

```{r}
# read in data sets
# Download file from github and place in same folder

troll_tweets<- read.csv("IRAhandle_tweets_12.csv")

```

Variable documentation can be found on [Github](https://github.com/fivethirtyeight/russian-troll-tweets/)

**Data Exploration: size, distribution, etc**

```{r}
# look at the dimensions
dim(troll_tweets)


```

**Q: Data Cleaning**

```{r}
# only consider tweets that are in English 
troll_tweets1 <- troll_tweets%>%
  filter(language == 'English')
  

```

#should we just ask them to make two or three exploratory plots of variables they find interesting?
```{r}
#possible ideas for exploratory plots

#see where tweets were being posted from
ggplot(troll_tweets1, aes(x = region)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 90))

#see what kinds of accounts there are
ggplot(troll_tweets1, aes(x = account_category)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 90))
```

#unnest the tokens so that instead of each row being a tweet, each row is a word
```{r, eval=TRUE}
#separate tweet so that each row gets an individual word: unnest tokens
troll_tweets_untoken <- troll_tweets1 %>%
  unnest_tokens(word,content)

troll_tweets_untoken

#get rid of stopwords (the, and, etc.)
troll_tweets_cleaned <- troll_tweets_untoken %>%
  anti_join(stop_words) %>%
  filter(word != 'https', word != 't.co') #get rid of https and t.co

# top 100 words 
troll_tweets_cleaned %>%
  count(word) %>%
  slice_max(order_by = n, n = 100)
```

```{r}
# Final cleaning stage: get rid of 2, rt, 1, amp, 3, http, 5, 4, 10, and single letter words
troll_tweets_fullclean <- troll_tweets_cleaned %>%
  #mutate(word = str_extract(word,"[a-z']+")) %>%
  filter(word != '1', word != '2', word != '3', word != '4', word != '5', word != '10', word != 'rt', word != 'amp', word != 'http', !(word %in% letters))
```

```{r}
troll_tweets_small <- troll_tweets_fullclean %>%
  count(word) %>%
  slice_max(order_by = n, n = 50)

ggplot(troll_tweets_small, aes(y = fct_reorder(word,n), x = n)) +
  geom_col()
```

**Q: Make wordcloud**
```{r message = FALSE, warning = FALSE}
troll_tweets_100 <- troll_tweets_fullclean %>%
  count(word) %>%
  slice_max(order_by = n, n = 100)

# most frequently used words
troll_tweets_100 %>%
  with(wordcloud(word, n, max.words = 100))

wordcloud(words = troll_tweets_100$word, freq = troll_tweets_100$n, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))


# sentiment analysis (positive vs negative)
troll_tweets_100 %>%
  inner_join(get_sentiments("bing")) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("red", "purple"),
                   max.words = 100)
```

**Q: More Sentiment Analysis**

```{r}
# bing makes it into positive / negative
troll_tweets_sentiment <- troll_tweets_fullclean %>%
  inner_join(get_sentiments("bing")) %>%
  count(sentiment)

#count negative and positive words
troll_tweets_sentiment

get_sentiments("bing")
```




